{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from opera import Mixture\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from numpy import linalg as LA\n",
        "from copy import copy\n",
        "from opera import Mixture\n",
        "from matplotlib.ticker import ScalarFormatter\n",
        "from sklearn.impute import SimpleImputer\n",
        "from mkl_algorithms import Raker\n",
        "from mkl_algorithms import OMKLGF\n",
        "\n",
        "data = np.genfromtxt('Bias_correction_ucl.csv', skip_header=1, delimiter=',')\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer = imputer.fit(data)\n",
        "data = imputer.transform(data)\n",
        "X = data[:,2:-2]\n",
        "Y = data[:,-1:]\n",
        "\n",
        "\n",
        "# Normalizing the target vector Y\n",
        "Y = (Y / (np.max(Y) - np.min(Y))) - np.min(Y) / (np.max(Y) - np.min(Y))\n",
        "\n",
        "# Normalizing the feature matrix X\n",
        "M, N = X.shape\n",
        "X_norms = np.linalg.norm(X, axis=1) #Calculate the norms of each row\n",
        "X = X / np.max(X_norms) # Divide each row by the maximum norm\n",
        "\n",
        "# Creating lists of gammas and kernels for feature transformation\n",
        "gamma = []\n",
        "kernel_list = []\n",
        "num_rbf = 51\n",
        "for i in range(num_rbf):\n",
        "    gamma.append(10 ** (4 * (i / 50) - 2))\n",
        "    kernel_list.append('Gaussian')\n",
        "num_lap = 25\n",
        "for i in range(num_lap):\n",
        "    gamma.append(10 ** ((i / 6) - 2))\n",
        "    kernel_list.append('Laplacian')\n",
        "gamma = np.array(gamma)\n",
        "n_components = 50  # Number of random features per kernel\n",
        "P = num_rbf + num_lap  # Total number of kernels\n",
        "\n",
        "# Initialize lists to store MSE, predictions and weights for different algorithms\n",
        "mse_vaw2 = []\n",
        "mse_vaw2_trunc = []\n",
        "mse_aggr = []\n",
        "mse_ewaf = []\n",
        "mse_boa = []\n",
        "mse_mlpol = []\n",
        "mse_mlprod = []\n",
        "mse_vaw_all = []\n",
        "mse_OGD_tuned = np.inf\n",
        "mse_vaw2_5kernels = []\n",
        "mse_omkl = []\n",
        "mse_raker = []\n",
        "\n",
        "\n",
        "cc_predictions_vaw2 = np.zeros((5, Y.shape[0]))\n",
        "cc_predictions_aggr = np.zeros((5, Y.shape[0]))\n",
        "cc_prediction_ewa_05 = np.zeros((5, Y.shape[0]))\n",
        "cc_predictions_boa = np.zeros((5, Y.shape[0]))\n",
        "cc_predictions_mlpol = np.zeros((5, Y.shape[0]))\n",
        "cc_predictions_mlprod = np.zeros((5, Y.shape[0]))\n",
        "cc_predictions_omkl = []\n",
        "cc_predictions_raker = []\n",
        "\n",
        "cc_mse_vaw2 = np.zeros((5, Y.shape[0]))\n",
        "cc_mse_aggr = np.zeros((5, Y.shape[0]))\n",
        "cc_mse_ewa_05 = np.zeros((5, Y.shape[0]))\n",
        "cc_mse_boa = np.zeros((5, Y.shape[0]))\n",
        "cc_mse_mlpol = np.zeros((5, Y.shape[0]))\n",
        "cc_mse_mlprod = np.zeros((5, Y.shape[0]))\n",
        "cc_mse_omkl = np.zeros((5, Y.shape[0]))\n",
        "cc_mse_raker = np.zeros((5, Y.shape[0]))\n",
        "\n",
        "weights_vaw2 = np.zeros((5, P))\n",
        "weights_ewa = np.zeros((5, P))\n",
        "weights_mlprod = np.zeros((5, P))\n",
        "\n",
        "\n",
        "def generate_random_features_dict_ran(X, ran_feature, gamma, kernel_list):\n",
        "    \"\"\"\n",
        "    Generates random features using Fourier features with given kernels and gammas.\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray): Feature matrix.\n",
        "        ran_feature (np.ndarray): Random feature matrix.\n",
        "        gamma (np.ndarray): Gamma values for kernels.\n",
        "        kernel_list (list): List of kernel names.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of random features for each kernel.\n",
        "    \"\"\"\n",
        "    M, N = X.shape\n",
        "    _, n_components, b = ran_feature.shape\n",
        "\n",
        "    random_features = {}\n",
        "\n",
        "    for i, kernel_type in enumerate(kernel_list):\n",
        "        features = np.zeros((M, n_components * 2))  # Features for current kernel\n",
        "        for j in range(M):\n",
        "            X_f = X[j:j + 1, :].dot(ran_feature[:, :, i])\n",
        "            features[j, :] = (1 / np.sqrt(n_components)) * np.concatenate((np.sin(X_f), np.cos(X_f)), axis=1)\n",
        "        random_features[f\"{kernel_type}_{i}\"] = features\n",
        "\n",
        "    return random_features\n",
        "\n",
        "def vaw_forecaster(features, target, lambda_reg=1, weights=None):\n",
        "\n",
        "    \"\"\"\n",
        "    Vovk-Azoury-Warmuth forecaster with closed-form solution using Sherman-Morrison update.\n",
        "\n",
        "    Args:\n",
        "        features (np.ndarray): Feature matrix, where each row is a feature vector (shape: n_samples, n_features).\n",
        "        target (np.ndarray): Target vector, with target values (shape: n_samples).\n",
        "        lambda_reg (float): Regularization parameter (default: 1.0).\n",
        "        weights (np.ndarray): Initial weights vector (default: None, initialized to zeros).\n",
        "\n",
        "    Returns:\n",
        "        tuple: predictions, rmse_history, updated weights.\n",
        "    \"\"\"\n",
        "\n",
        "    n_samples, n_features = features.shape\n",
        "    predictions = []\n",
        "    rmse_history = []  # To store RMSE at each horizon\n",
        "    squared_errors_cumulative = 0  # Cumulative squared errors\n",
        "\n",
        "    # Initialize weights if not provided, else use provided weights\n",
        "    if weights is None:\n",
        "        weights = np.zeros(n_features)\n",
        "\n",
        "    # Initialize the inverse of (lambda * I + sum of zi zi^T), using first the value lambda * I\n",
        "    reg_matrix_inverse = (1/lambda_reg) * np.eye(n_features) # S^-1\n",
        "\n",
        "    # Initialize w, which is the result of sum(yizi)\n",
        "    sum_yizi = np.zeros(n_features)\n",
        "\n",
        "    for t in range(n_samples):\n",
        "        zt = features[t]  # Current feature vector\n",
        "        yt = target[t]    # Current target value\n",
        "\n",
        "        # Compute the prediction, since x_t = reg_matrix_inverse * sum_yizi\n",
        "        if t==0:\n",
        "             prediction = 0\n",
        "        else:\n",
        "            prediction = np.dot(sum_yizi, reg_matrix_inverse @ zt) # This is xt^T * zt\n",
        "\n",
        "        predictions.append(prediction)\n",
        "\n",
        "        # Compute squared error\n",
        "        squared_error = (yt - prediction) ** 2\n",
        "        squared_errors_cumulative += squared_error\n",
        "\n",
        "        # Calculate RMSE up to the current horizon\n",
        "        rmse_t = np.sqrt(squared_errors_cumulative / (t + 1))\n",
        "        rmse_history.append(rmse_t)\n",
        "\n",
        "        # Update sum_yizi\n",
        "        sum_yizi += yt*zt\n",
        "\n",
        "        # Update the inverse matrix using Sherman-Morrison formula:\n",
        "        # S_t = S_{t-1} + z_t z_t^T\n",
        "        # S_t^-1 = S_{t-1}^-1 - (S_{t-1}^-1 @ z_t @ z_t^T @ S_{t-1}^-1) / (1 + z_t^T @ S_{t-1}^-1 @ z_t)\n",
        "        zt = zt.reshape(-1, 1) # Convert to a column vector\n",
        "        numerator = reg_matrix_inverse @ zt @ zt.T @ reg_matrix_inverse\n",
        "        denominator = 1 + zt.T @ reg_matrix_inverse @ zt\n",
        "        reg_matrix_inverse = reg_matrix_inverse - (numerator/ denominator[0,0])\n",
        "\n",
        "    # Get the last weight vector\n",
        "    if n_samples == 0:\n",
        "        weights = np.zeros(n_features)\n",
        "    else:\n",
        "        weights = reg_matrix_inverse @ sum_yizi\n",
        "\n",
        "    return predictions, rmse_history, weights\n",
        "\n",
        "def quadratic_loss_gradient(w, x_i, y_i):\n",
        "    \"\"\"\n",
        "    Computes the gradient of the quadratic loss function.\n",
        "\n",
        "    Args:\n",
        "        w (np.ndarray): Weight vector.\n",
        "        x_i (np.ndarray): Input feature vector.\n",
        "        y_i (float): Target value.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Gradient of the loss function.\n",
        "    \"\"\"\n",
        "    return 2 * (np.dot(w, x_i) - y_i) * x_i\n",
        "\n",
        "def online_gradient_descent(X, y, learning_rates=None, initial_w=None):\n",
        "    \"\"\"\n",
        "    Implements online gradient descent.\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray): Feature matrix. Shape (m, n).\n",
        "        y (np.ndarray): Target vector. Shape (m,).\n",
        "        learning_rates (list or np.ndarray): Learning rates (eta_t). If None, uses 1/m.\n",
        "        initial_w (np.ndarray): Initial weight vector. If None, uses a vector of zeros.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (w, predictions), where w is the array of weights w_t, predictions is the array of predictions.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the lengths of X and y do not match.\n",
        "    \"\"\"\n",
        "    if len(X) != len(y):\n",
        "        raise ValueError(\"Lengths of X and y must match.\")\n",
        "\n",
        "    m = len(y)  # Number of samples\n",
        "    n = X.shape[1] if X.ndim > 1 else 1  # Number of features\n",
        "\n",
        "    if initial_w is None:\n",
        "        initial_w = np.zeros(n)\n",
        "    else:\n",
        "        if len(initial_w) != n:\n",
        "            raise ValueError(\"Dimension of initial_w must match the number of features\")\n",
        "\n",
        "    if learning_rates == 'descending':\n",
        "        learning_rates = [0.1 / np.sqrt(t + 1) for t in range(m)]\n",
        "    elif learning_rates is None:\n",
        "        learning_rates = np.full(m, 1/m) #Default learning rate\n",
        "    else:\n",
        "        learning_rates = np.full(m, learning_rates) #If learning_rates is a single value, use it for all steps.\n",
        "\n",
        "    if X.ndim == 1:\n",
        "        X = X.reshape(-1, 1)  # Convert X to a 2D array\n",
        "\n",
        "    w = np.zeros((m, n))\n",
        "    w[0] = initial_w.copy()\n",
        "    predictions = np.zeros(m)\n",
        "\n",
        "    for t in range(m - 1):\n",
        "        predictions[t] = np.dot(w[t], X[t]) if w[t].ndim == 1 else np.dot(w[t], X[t].reshape(-1, 1))\n",
        "        gradient = quadratic_loss_gradient(w[t], X[t], y[t])\n",
        "        w[t + 1] = w[t] - learning_rates[t] * gradient\n",
        "    predictions[m - 1] = np.dot(w[m - 1], X[m - 1]) if w[m - 1].ndim == 1 else np.dot(w[m - 1], X[m - 1].reshape(-1, 1))\n",
        "\n",
        "    return w, predictions\n",
        "\n",
        "# Generating expert predictions using VAW on transformed features\n",
        "all_predictions_df_lst = []\n",
        "for cc in range(5):\n",
        "    ran_feature = np.zeros((N, n_components, gamma.shape[0]))\n",
        "    for i in range(num_rbf):\n",
        "        ran_feature[:, :, i] = np.random.randn(N, n_components) * np.sqrt(1 / gamma[i])\n",
        "    for i in range(num_lap):\n",
        "        ran_feature[:, :, i + num_rbf] = np.random.standard_cauchy((N, n_components)) * (1 / gamma[i + num_rbf])\n",
        "    fourier_features = generate_random_features_dict_ran(X, ran_feature, gamma, kernel_list)\n",
        "    all_predictions_df = pd.DataFrame()\n",
        "    rmse_individual = []\n",
        "    for kernel_name, features in fourier_features.items():\n",
        "        predictions, rmse_history, _ = vaw_forecaster(\n",
        "            fourier_features[kernel_name],\n",
        "            Y,\n",
        "            lambda_reg=1.\n",
        "        )\n",
        "        rmse_individual.append(rmse_history[-1])\n",
        "        predictions_df = pd.DataFrame(predictions, columns=[kernel_name])\n",
        "        all_predictions_df = pd.concat([all_predictions_df, predictions_df], axis=1)\n",
        "    all_predictions_df_lst.append(all_predictions_df)\n",
        "\n",
        "# Truncating expert predictions to the range of Y\n",
        "all_predictions_df_lst_trunc = [df.map(lambda u: min(max(u, np.min(Y)), np.max(Y))) for df in copy(all_predictions_df_lst)]\n",
        "\n",
        "#Raker\n",
        "lam = .001\n",
        "\n",
        "for cc in range(0,5):\n",
        "    # Generating Random Features\n",
        "    ran_feature = np.zeros((N,n_components,gamma.shape[0]))\n",
        "    for i in range(0,num_rbf):\n",
        "        ran_feature[:,:,i] = np.random.randn(N,n_components)*np.sqrt(1/gamma[i])\n",
        "    for i in range(0,num_lap):\n",
        "        ran_feature[:,:,i+num_rbf] = np.random.standard_cauchy((N,n_components))*(1/gamma[i+num_rbf])\n",
        "\n",
        "    e = np.zeros((M,1))\n",
        "    w = np.ones((1,gamma.shape[0]))\n",
        "    theta = np.zeros((2*n_components,gamma.shape[0]))\n",
        "    predictions = []\n",
        "    for i in range(0,M):\n",
        "        eta = .1/np.sqrt(i+1)\n",
        "        raker = Raker(lam, ran_feature, eta)\n",
        "        f_RF, f_RF_p, X_features = raker.predict(X[i:i+1,:], theta, w)\n",
        "        w, theta = raker.update(f_RF_p, Y[i], theta, w, X_features)\n",
        "        predictions.append(f_RF)\n",
        "    try:\n",
        "      mse_raker.append(MSE(np.array(predictions).ravel(), Y))\n",
        "    except ValueError:\n",
        "        continue\n",
        "    cc_predictions_raker.append(predictions)\n",
        "cc_predictions_raker = np.array(cc_predictions_raker)\n",
        "\n",
        "\n",
        "#OMKL-GF\n",
        "for cc in range(0,5):\n",
        "    # Generating Random Features\n",
        "    ran_feature = np.zeros((N,n_components,gamma.shape[0]))\n",
        "    for i in range(0,num_rbf):\n",
        "        ran_feature[:,:,i] = np.random.randn(N,n_components)*np.sqrt(1/gamma[i])\n",
        "    for i in range(0,num_lap):\n",
        "        ran_feature[:,:,i+num_rbf] = np.random.standard_cauchy((N,n_components))*(1/gamma[i+num_rbf])\n",
        "\n",
        "    # OMKL-GF with M = 5 and J = 2\n",
        "    e = np.zeros((M,1))\n",
        "    w = np.ones((1,gamma.shape[0]))\n",
        "    theta = np.zeros((2*n_components,gamma.shape[0]))\n",
        "    m = 5\n",
        "    J = 2\n",
        "    ter = 0\n",
        "    p_c = np.zeros((1,J))\n",
        "    A_t = np.zeros((gamma.shape[0],J))\n",
        "    q = np.zeros((gamma.shape[0],1))\n",
        "    predictions = []\n",
        "    for i in range(0,M):\n",
        "        eta = .1/np.sqrt(i+1)\n",
        "        omklgf = OMKLGF(lam, ran_feature, gamma, eta, eta, m, J)\n",
        "        if ter == 0:\n",
        "            A_t, p_kk = omklgf.graph_gen(w)\n",
        "        f_RF, f_RF_p, X_features, n_n, p_c, I_t = omklgf.predict(X[i:i+1,:], theta, w, A_t)\n",
        "        w, theta = omklgf.update(f_RF_p, Y[i], theta, w, X_features, n_n, p_kk, p_c, ter)\n",
        "        predictions.append(f_RF)\n",
        "        if i>300:\n",
        "            ter = 1\n",
        "        try:\n",
        "          mse_omkl.append(MSE(np.array(predictions).ravel(), Y))\n",
        "        except ValueError:\n",
        "            continue\n",
        "    cc_predictions_omkl.append(predictions)\n",
        "cc_predictions_omkl = np.array(cc_predictions_omkl)\n",
        "\n",
        "# VAW^2 algorithm\n",
        "for cc in range(5):\n",
        "    predictions, _, weights_vaw2[cc] = vaw_forecaster(np.array(all_predictions_df_lst[cc]), Y, lambda_reg=1.)\n",
        "    cc_predictions_vaw2[cc] = predictions\n",
        "    mse_vaw2.append(MSE(predictions, Y))\n",
        "\n",
        "# VAW^2 algorithm (with truncated expert predictions)\n",
        "\n",
        "for cc in range(5):\n",
        "    predictions, _, weights_vaw2[cc] = vaw_forecaster(np.array(all_predictions_df_lst_trunc[cc]), Y, lambda_reg=1.)\n",
        "    mse_vaw2_trunc.append(MSE(predictions, Y))\n",
        "\n",
        "# Aggregating algorithm of Vovk\n",
        "for cc in range(5):\n",
        "    x_data = all_predictions_df_lst_trunc[cc].values\n",
        "    y_data = Y\n",
        "    w_out = np.zeros((M, P))  # Initialize weights for inner and outer VAW algorithms\n",
        "    eta = 2\n",
        "\n",
        "    # Define the loss function\n",
        "    loss = lambda y1, y2: (y1 - y2) ** 2\n",
        "\n",
        "    # Cumulative losses of N experts\n",
        "    L = np.zeros((P, M))\n",
        "    # Predictions of the cumulative algorithm\n",
        "    prediction = np.zeros(M)\n",
        "    # Potential\n",
        "    r = np.zeros(M)\n",
        "\n",
        "    for t in range(M - 1):\n",
        "        loss_experts_1 = loss(x_data[t + 1], np.ones(P)) # loss when r = 1\n",
        "        loss_experts_0 = loss(x_data[t + 1], np.zeros(P)) # loss when r = 0\n",
        "\n",
        "        r[t + 1] = 0.5 + (1 / (2 * eta)) * np.log(\n",
        "            np.sum(np.exp(-eta * L[:, t] - eta * loss_experts_1)) / np.sum(np.exp(-eta * L[:, t] - eta * loss_experts_0))\n",
        "        )\n",
        "\n",
        "        if r[t + 1] < 0:\n",
        "            prediction[t + 1] = 0\n",
        "        elif 0 <= r[t + 1] <= 1:\n",
        "            prediction[t + 1] = r[t + 1]\n",
        "        else:\n",
        "            prediction[t + 1] = 1\n",
        "\n",
        "        L[:, t + 1] = L[:, t] + loss(x_data[t + 1], np.tile(y_data[t + 1], P))\n",
        "    try:\n",
        "        mse_aggr.append(MSE(prediction, y_data))\n",
        "    except ValueError:\n",
        "        continue\n",
        "    cc_predictions_aggr[cc] = prediction\n",
        "\n",
        "# VAW-EWA algorithm\n",
        "eta = 0.5\n",
        "for cc in range(5):\n",
        "    x_data = all_predictions_df_lst_trunc[cc].values\n",
        "    y_data = Y\n",
        "\n",
        "    w_out = np.zeros((M, P))  # Initialize weights\n",
        "\n",
        "    # Define the loss function\n",
        "    loss = lambda y1, y2: (y1 - y2) ** 2\n",
        "\n",
        "    # Cumulative losses of N experts\n",
        "    L = np.zeros((P, M))\n",
        "    # Predictions of the cumulative algorithm\n",
        "    prediction = np.zeros(M)\n",
        "    # Potential\n",
        "    r = np.zeros(M)\n",
        "\n",
        "    for t in range(M - 1):\n",
        "\n",
        "        # EWA prediction\n",
        "        prediction[t + 1] = np.sum(np.exp(-eta * L[:, t]) * x_data[t + 1]) / np.sum(np.exp(-eta * L[:, t]))\n",
        "\n",
        "        L[:, t + 1] = L[:, t] + loss(x_data[t + 1], np.tile(y_data[t + 1], P))\n",
        "    weights_ewa[cc] = (np.exp(-eta * L[:, -1]) / np.sum(np.exp(-eta * L[:, -1])))\n",
        "    try:\n",
        "        mse_ewaf.append(MSE(prediction, y_data))\n",
        "    except ValueError:\n",
        "        continue\n",
        "    cc_prediction_ewa_05[cc] = prediction\n",
        "\n",
        "# VAW-BOA algorithm\n",
        "for cc in range(5):\n",
        "    model = 'BOA'\n",
        "    mod_opera = Mixture(\n",
        "        y=Y.ravel(),\n",
        "        experts=all_predictions_df_lst_trunc[cc],\n",
        "        model=model,\n",
        "        loss_type=\"mse\",\n",
        "        loss_gradient=False,\n",
        "    )\n",
        "    try:\n",
        "        mse_boa.append(MSE(Y, mod_opera.predictions))\n",
        "    except ValueError:\n",
        "        continue\n",
        "    cc_predictions_boa[cc] = mod_opera.predictions\n",
        "\n",
        "# VAW-ML-Poly algorithm\n",
        "for cc in range(5):\n",
        "    model = 'MLpol'\n",
        "    mod_opera = Mixture(\n",
        "        y=Y.ravel(),\n",
        "        experts=all_predictions_df_lst[cc],\n",
        "        model=model,\n",
        "        loss_type=\"mse\",\n",
        "        loss_gradient=False,\n",
        "    )\n",
        "    try:\n",
        "        mse_mlpol.append(MSE(Y, mod_opera.predictions))\n",
        "    except ValueError:\n",
        "        continue\n",
        "    cc_predictions_mlpol[cc] = mod_opera.predictions\n",
        "\n",
        "# VAW-ML-Prod algorithm\n",
        "for cc in range(5):\n",
        "    model = 'MLprod'\n",
        "    mod_opera = Mixture(\n",
        "        y=Y.ravel(),\n",
        "        experts=all_predictions_df_lst[cc],\n",
        "        model=model,\n",
        "        loss_type=\"mse\",\n",
        "        loss_gradient=False,\n",
        "    )\n",
        "    weights_mlprod[cc] = mod_opera.w\n",
        "    try:\n",
        "        mse_mlprod.append(MSE(Y, mod_opera.predictions))\n",
        "    except ValueError:\n",
        "        continue\n",
        "    cc_predictions_mlprod[cc] = mod_opera.predictions\n",
        "\n",
        "\n",
        "# --- Experiments with concatenated feature vectors ---\n",
        "\n",
        "\n",
        "kernel_list = []\n",
        "gamma = np.array([0.01, 0.1, 1, 10, 100])\n",
        "kernel_list = ['Gaussian'] * 5\n",
        "\n",
        "# VAW with concatenated Fourier features\n",
        "for cc in range(5):\n",
        "    ran_feature = np.zeros((N,n_components,gamma.shape[0]))\n",
        "    for i in range(0,gamma.shape[0]):\n",
        "        ran_feature[:,:,i] = np.random.randn(N,n_components)*np.sqrt(1/gamma[i])\n",
        "    fourier_features = generate_random_features_dict_ran(X, ran_feature, gamma, kernel_list)\n",
        "\n",
        "    x_data = np.array(list(fourier_features.values()))\n",
        "    x_data = x_data.transpose(1, 0, 2).reshape(x_data.shape[1], x_data.shape[0]*x_data.shape[2])\n",
        "    predictions, _, _ = vaw_forecaster(x_data, Y, lambda_reg=1)\n",
        "    try:\n",
        "        mse_vaw_all.append(MSE(predictions, Y))\n",
        "    except ValueError:\n",
        "        continue\n",
        "\n",
        "# OGD with tuned step size\n",
        "for lr in [0.001, 0.01, 0.1, 'descending']:\n",
        "    mse_OGD = []\n",
        "    for cc in range(5):\n",
        "        ran_feature = np.zeros((N,n_components,gamma.shape[0]))\n",
        "        for i in range(0,gamma.shape[0]):\n",
        "            ran_feature[:,:,i] = np.random.randn(N,n_components)*np.sqrt(1/gamma[i])\n",
        "        fourier_features = generate_random_features_dict_ran(X, ran_feature, gamma, kernel_list)\n",
        "\n",
        "        x_data = np.array(list(fourier_features.values()))\n",
        "        x_data = x_data.transpose(1, 0, 2).reshape(x_data.shape[1], x_data.shape[0]*x_data.shape[2])\n",
        "        _, OGD_predictions = online_gradient_descent(x_data, Y, learning_rates=lr)\n",
        "        mse_OGD.append(MSE(OGD_predictions, Y))\n",
        "    if np.mean(mse_OGD) < mse_OGD_tuned:\n",
        "        mse_OGD_tuned = np.mean(mse_OGD)\n",
        "\n",
        "\n",
        "# VAW^2 with 5 Kernels\n",
        "\n",
        "# Generate VAW predictions with 5 kernels\n",
        "all_predictions_df_lst = []\n",
        "N = X.shape[1]\n",
        "for cc in range(5):\n",
        "    ran_feature = np.zeros((N, n_components, len(gamma)))\n",
        "    for i in range(len(gamma)):\n",
        "        ran_feature[:, :, i] = np.random.randn(N, n_components) * np.sqrt(1 / gamma[i])\n",
        "    fourier_features = generate_random_features_dict_ran(X, ran_feature, gamma, kernel_list)\n",
        "    all_predictions_df = pd.DataFrame()\n",
        "    for kernel_name, features in fourier_features.items():\n",
        "        predictions, _, _ = vaw_forecaster(features, Y, lambda_reg=1.)\n",
        "        predictions_df = pd.DataFrame(predictions, columns=[kernel_name])\n",
        "        all_predictions_df = pd.concat([all_predictions_df, predictions_df], axis=1)\n",
        "    all_predictions_df_lst.append(all_predictions_df)\n",
        "\n",
        "    predictions, _, _ = vaw_forecaster(np.array(all_predictions_df_lst[cc]), Y, lambda_reg=1.) # VAW^2\n",
        "    mse_vaw2_5kernels.append(MSE(predictions, Y))\n",
        "\n",
        "# Calculate cumulative MSE for each algorithm\n",
        "for cc in range(5):\n",
        "    for t in range(Y.shape[0]):\n",
        "        cc_mse_vaw2[cc, t] = 1. / (t + 1) * MSE(cc_predictions_vaw2[cc][:t + 1], Y[:t + 1])\n",
        "        cc_mse_aggr[cc, t] = 1. / (t + 1) * MSE(cc_predictions_aggr[cc][:t + 1], Y[:t + 1])\n",
        "        cc_mse_ewa_05[cc, t] = 1. / (t + 1) * MSE(cc_prediction_ewa_05[cc][:t + 1], Y[:t + 1])\n",
        "        cc_mse_boa[cc, t] = 1. / (t + 1) * MSE(cc_predictions_boa[cc][:t + 1], Y[:t + 1])\n",
        "        cc_mse_mlpol[cc, t] = 1. / (t + 1) * MSE(cc_predictions_mlpol[cc][:t + 1], Y[:t + 1])\n",
        "        cc_mse_mlprod[cc, t] = 1. / (t + 1) * MSE(cc_predictions_mlprod[cc][:t + 1], Y[:t + 1])\n",
        "        cc_mse_omkl[cc, t] = 1./(t+1)*MSE(cc_predictions_omkl[cc][:t+1].ravel(), Y[:t+1])\n",
        "        cc_mse_raker[cc, t] = 1./(t+1)*MSE(cc_predictions_raker[cc][:t+1].ravel(), Y[:t+1])\n",
        "\n",
        "# Calculate mean cumulative MSE across all repetitions\n",
        "cc_mse_vaw2 = np.mean(cc_mse_vaw2, axis=0)\n",
        "cc_mse_aggr = np.mean(cc_mse_aggr, axis=0)\n",
        "cc_mse_ewa_05 = np.mean(cc_mse_ewa_05, axis=0)\n",
        "cc_mse_boa = np.mean(cc_mse_boa, axis=0)\n",
        "cc_mse_mlpol = np.mean(cc_mse_mlpol, axis=0)\n",
        "cc_mse_mlprod = np.mean(cc_mse_mlprod, axis=0)\n",
        "cc_mse_omkl = np.mean(cc_mse_omkl, axis = 0)\n",
        "cc_mse_raker = np.mean(cc_mse_raker, axis = 0)\n",
        "\n",
        "# Print final MSE values\n",
        "print('MSE of Raker is ', np.mean(mse_raker))\n",
        "print('MSE of OMKL-GF is ', np.mean(mse_omkl))\n",
        "print('MSE of VAW^2 is ', np.mean(mse_vaw2))\n",
        "print('MSE of VAW^2 with truncated expert predictions is ', np.mean(mse_vaw2_trunc))\n",
        "print('MSE of VAW-AGGR is ', np.mean(mse_aggr))\n",
        "print('MSE of VAW-EWA is ', np.mean(mse_ewaf))\n",
        "print('MSE of VAW-BOA is ', np.mean(mse_boa))\n",
        "print('MSE of VAW-ML-Poly is ', np.mean(mse_mlpol))\n",
        "print('MSE of VAW-ML-Prod is ', np.mean(mse_mlprod))\n",
        "print('MSE of VAW with concatenated features: ', np.mean(mse_vaw_all))\n",
        "print('MSE of OGD with tuned stepsize is ', mse_OGD_tuned)\n",
        "print('MSE of VAW^2 on 5 kernels is ', np.mean(mse_vaw2_5kernels))\n",
        "\n",
        "\n",
        "# Plotting cumulative MSE for different algorithms\n",
        "start_index = 700\n",
        "fig1 = plt.figure(1)\n",
        "ax = fig1.add_subplot()\n",
        "plt.title('Bias')\n",
        "plt.xlabel('Iteration number')\n",
        "plt.ylabel('MSE')\n",
        "sf = ScalarFormatter()\n",
        "sf.set_powerlimits((-4, 4))\n",
        "ax.yaxis.set_major_formatter(sf)\n",
        "ax.plot(np.arange(start_index, Y.shape[0]), cc_mse_raker[start_index:], 'b', label = 'Raker')\n",
        "ax.plot(np.arange(start_index, Y.shape[0]), cc_mse_omkl[start_index:], 'g:', label = 'OMKL-GF')\n",
        "ax.plot(np.arange(start_index, Y.shape[0]), cc_mse_aggr[start_index:], 'b:', label='VAW-Aggr')\n",
        "ax.plot(np.arange(start_index, Y.shape[0]), cc_mse_ewa_05[start_index:], 'g', label='VAW-EWA')\n",
        "ax.plot(np.arange(start_index, Y.shape[0]), cc_mse_mlprod[start_index:], 'k', label='VAW-ML-Prod')\n",
        "ax.plot(np.arange(start_index, Y.shape[0]), cc_mse_vaw2[start_index:], 'r', label=r'VAW$^2$')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting weights of different algorithms\n",
        "fig2 = plt.figure(2)\n",
        "ax = fig2.add_subplot()\n",
        "P = num_rbf + num_lap\n",
        "ax.plot(np.arange(1, P + 1), np.mean(weights_vaw2, axis = 0), 'r', label=r'VAW$^2$')\n",
        "ax.plot(np.arange(1, P + 1), np.mean(weights_ewa, axis = 0), 'g', label='VAW-EWA')\n",
        "ax.plot(np.arange(1, P + 1), np.mean(weights_mlprod, axis = 0), 'k', label='VAW-ML-Prod')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('Kernel')\n",
        "plt.ylabel('Weights')\n",
        "# Adding kernel numbers and names to the x-axis\n",
        "x_ticks = [1, 51, P]  # Kernel numbers to display\n",
        "x_labels = [str(tick) for tick in x_ticks]  # Convert kernel numbers to strings\n",
        "\n",
        "# Adding kernel names\n",
        "x_ticks_labels = [25, 60]  # Positions for kernel names\n",
        "x_labels_labels = ['Gaussian', 'Laplacian']  # Kernel names\n",
        "\n",
        "# Combining kernel numbers and names for x-axis ticks\n",
        "all_ticks = sorted(list(set(x_ticks + x_ticks_labels)))\n",
        "all_labels = []\n",
        "\n",
        "# Creating labels for all ticks\n",
        "for tick in all_ticks:\n",
        "    if tick in x_ticks:\n",
        "        all_labels.append(str(tick))  # Add kernel number\n",
        "    elif tick in x_ticks_labels:\n",
        "        all_labels.append(x_labels_labels[x_ticks_labels.index(tick)])  # Add kernel name\n",
        "\n",
        "plt.xticks(all_ticks, all_labels)\n",
        "plt.title('Bias')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p2vxYzZ0ZIB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yAtfGB_Ky5y8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}